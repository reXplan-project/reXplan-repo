{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI supported Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuNet(object):                                                               # to the class we shall provide a model, a loss_fn and an optimizer.\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)                                                  # here we send the model to the device\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "    def to(self, device):                                                           # this is the function sending the model to the device\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):                           # data loaders provide the input data in a sutiable format to the model, in a minibatch size\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()                                                      # the model has a different behaviour during training and evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()                                              # avoid cumulation of gradients\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()                                                       # here we set the model to evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, \n",
    "            # since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the \n",
    "        # same mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):                                             # this function execute the training of the model\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:                                                         # this is optional, i.e. Tensorboard output\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))                                 # sending input to device\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()                                          # sending back to cpu for return\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b', lw=1)\n",
    "        plt.plot(self.val_losses, label='Test Loss', c='r', lw=1)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PATH_NETWORK_WG = r\".\\network_with_gen.csv\" \n",
    "# df_I = pd.read_csv(PATH_NETWORK_WG, sep=\";\", decimal=\",\")\n",
    "# temp_df = df_I.copy()\n",
    "# arr = temp_df.to_numpy()\n",
    "# df_I = pd.DataFrame(np.tile(arr, (240, 1)), columns = temp_df.columns)\n",
    "\n",
    "PATH_MONTECARLO = r\".\\montecarlo_database.csv\" \n",
    "df_montecarlo = pd.read_csv(PATH_MONTECARLO, sep=\",\", index_col=[0, 1, 2, 3, 4])# , decimal=\",\")\n",
    "# number_of_strata = df_montecarlo['strata'].max()\n",
    "# number_of_iterations = df_montecarlo['iteration'].max()+1\n",
    "\n",
    "PATH_NETWORK = r\".\\network.xlsx\" \n",
    "df_network = pd.read_excel(PATH_NETWORK, sheet_name=\"profiles\", decimal=\",\")\n",
    "df_network = df_network.drop(index=0).reset_index(drop=True)\n",
    "df_network = df_network.drop(df_network.columns[0], axis=1)\n",
    "temp_df = df_network.copy()\n",
    "arr = temp_df.to_numpy()\n",
    "arr = arr.astype(np.float64)\n",
    "df_network = pd.DataFrame(np.tile(arr, (240, 1)), columns = temp_df.columns)\n",
    "\n",
    "PATH_ENGINE = r\".\\engine_database.csv\"\n",
    "df_engine = pd.read_csv(PATH_ENGINE, sep=\",\", index_col=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURRENTLY RECODED !!!\n",
    "X_df = df_engine.loc[:,:,\"in_service\",\"line\",:,:].stack().unstack(\"id\")\n",
    "number_of_lines = len(set(X_df.columns)) # number of lines\n",
    "df_network.index= X_df.index\n",
    "X_df = pd.concat([X_df, df_network], axis=1)\n",
    "###\n",
    "# Now the engine database is needed!\n",
    "###\n",
    "y_df = df_engine.loc[:,:,\"loss_of_load_p_mw\",\"load\",:,:].stack().unstack(\"id\")\n",
    "ysum_df = pd.DataFrame(y_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_montecarlo = df_montecarlo.stack().unstack(\"id\")\n",
    "df_network.index= X_df_montecarlo.index\n",
    "number_of_lines = len(set(X_df_montecarlo.columns))\n",
    "X_df_montecarlo = pd.concat([X_df_montecarlo, df_network], axis=1)\n",
    "X_df_montecarlo.insert(0, 'idx', range(1, len(X_df_montecarlo) + 1))\n",
    "X = X_df_montecarlo.to_numpy()\n",
    "\n",
    "\n",
    "z = pd.DataFrame(X).iloc[:,1:number_of_lines+1].astype(int).astype(str).agg(''.join, axis=1)\n",
    "l = []\n",
    "from collections import Counter\n",
    "c = Counter(z)\n",
    "for k in z:\n",
    "    if c[k] == 1:\n",
    "        l.append(str('G0'))\n",
    "    else:\n",
    "        l.append(str(k))\n",
    "\n",
    "X_train, X_val = train_test_split(X, train_size = 0.1, stratify = pd.DataFrame(l)[0], shuffle = True, random_state = 42)\n",
    "        \n",
    "###\n",
    "# Now the engine database is needed!\n",
    "# Step 1: Montecarlo Sampling\n",
    "# Step 2: Run OPF for selected 10%\n",
    "# Step 3: Populate Engine Database\n",
    "###\n",
    "\n",
    "# y_df = df_engine.loc[:,:,\"loss_of_load_p_mw\",\"load\",:,:].stack().unstack(\"id\")\n",
    "# ysum_df = pd.DataFrame(y_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>timestep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>239</td>\n",
       "      <td>2022-01-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>239</td>\n",
       "      <td>2022-01-01 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>239</td>\n",
       "      <td>2022-01-02 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>239</td>\n",
       "      <td>2022-01-02 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>239</td>\n",
       "      <td>2022-01-02 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iteration             timestep\n",
       "0             0  2022-01-01 01:00:00\n",
       "1             0  2022-01-01 03:00:00\n",
       "2             0  2022-01-01 13:00:00\n",
       "3             0  2022-01-01 17:00:00\n",
       "4             0  2022-01-02 00:00:00\n",
       "...         ...                  ...\n",
       "1195        239  2022-01-01 15:00:00\n",
       "1196        239  2022-01-01 18:00:00\n",
       "1197        239  2022-01-02 04:00:00\n",
       "1198        239  2022-01-02 10:00:00\n",
       "1199        239  2022-01-02 15:00:00\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "idx_for_opf = pd.Series(pd.DataFrame(X_train)[0])\n",
    "\n",
    "filtered_df = X_df_montecarlo[X_df_montecarlo['idx'].isin(idx_for_opf)]\n",
    "filtered_df = filtered_df.reset_index()\n",
    "filtered_df.rename(columns={'level_4': 'timestep'}, inplace=True)\n",
    "opfs = filtered_df[['iteration', 'timestep']]\n",
    "opfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE CORRECT VARIABLES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationSet = None, time = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Features and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X = X_df.to_numpy()\n",
    "y = ysum_df.to_numpy()\n",
    "# labels in str form ## sum(axis=1) gives different behaviour, dependent on python version (as it seems)\n",
    "z = pd.DataFrame(X).iloc[:,:number_of_lines].astype(int).astype(str).sum(axis=1)\n",
    "l = [] #  list\n",
    "c = Counter(z)\n",
    "for k in z:\n",
    "    #print(k)\n",
    "    if c[k] == 1:\n",
    "        l.append(str('G0'))\n",
    "    else:\n",
    "        l.append(str(k))\n",
    "\n",
    "# 1104 G0 values        \n",
    "#Split data Model ANN 1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1, stratify = pd.DataFrame(l)[0], shuffle=True, random_state=42)\n",
    "    \n",
    "##Split data Model ANN2\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X[(np.array(l) != \"G0\")], y[(np.array(l) != \"G0\")], train_size=0.1, stratify = pd.DataFrame(l)[(np.array(l) != \"G0\")][0], shuffle=True,random_state=42)\n",
    "# X_train = np.append(X_train, X[(np.array(l) == \"G0\")], axis=0)\n",
    "# y_train = np.append(y_train, y[(np.array(l) == \"G0\")], axis=0)\n",
    "\n",
    "#Split data Model ANN 3\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1,  \n",
    "#                                                 shuffle=True, random_state=42)\n",
    "\n",
    "# standardize data\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train = np.concatenate((X_train.T[:number_of_lines].T, scaler_x.fit_transform(X_train.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "X_val = np.concatenate((X_val.T[:number_of_lines].T, scaler_x.transform(X_val.T[number_of_lines:].T)), axis=1).astype(float)# without fit!\n",
    "y_train = scaler_y.fit_transform(y_train).astype(float)\n",
    "y_val = scaler_y.transform(y_val).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = pd.DataFrame(X_train).iloc[:,:number_of_lines].astype(int).astype(str).sum(axis=1) # labels in str form\n",
    "l_train = [] #  list\n",
    "c_train = Counter(z_train)\n",
    "for k in z_train:\n",
    "    #print(k)\n",
    "    l_train.append(k)\n",
    "len(set(l_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v2.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "torch.manual_seed(555)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).float()\n",
    "\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "\n",
    "dataset_train = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataset_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = []\n",
    "val_loader = []\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# # K-fold Cross Validation model evaluation\n",
    "# kfold = KFold(n_splits=5, shuffle=True)\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset_train)):\n",
    "\n",
    "#     # Print\n",
    "#     print(f'FOLD {fold}')\n",
    "#     print('--------------------------------')\n",
    "\n",
    "#     # Sample elements randomly from a given list of ids, no replacement.\n",
    "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "#     # Define data loaders for training and testing data in this fold\n",
    "#     train_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train, \n",
    "#                       batch_size=256, sampler=train_subsampler))\n",
    "#     val_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train,\n",
    "#                       batch_size=256, sampler=test_subsampler))\n",
    "\n",
    "   \n",
    "train_loader.append(DataLoader(dataset=dataset_train, batch_size=256, shuffle=True)) #batch_size=256\n",
    "val_loader.append(DataLoader(dataset=dataset_val, batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(X_train.shape[1], 512), nn.ReLU(),\n",
    "                      #nn.Dropout(p=0.1),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512,512), nn.ReLU(),\n",
    "                      nn.Linear(512, 256), nn.ReLU(),\n",
    "                      nn.Linear(256, y_train.shape[1]) ).to(device)\n",
    "\n",
    "# Defines an  optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet = NeuNet(model, loss_fn, optimizer)\n",
    "neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "neunet.set_tensorboard(name=\"runs\", folder = 'machine_learning_tutorial/Test_Grid10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(neunet.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet.train(n_epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig = neunet.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neunet.val_losses[-1])\n",
    "print(neunet.losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir machine_learning_tutorial/Test_Grid10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction of validation points\n",
    "i = 0\n",
    "start_t = 50\n",
    "end_t = start_t + 24*4*5\n",
    "#new_inputs = torch.tensor(X_val).float()\n",
    "new_inputs = np.array(X_val)\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = y_val\n",
    "f, (ax) = plt.subplots(1,1, figsize=(15, 10)) \n",
    "plt.plot(scaler_y.inverse_transform(orig)[start_t:end_t, i], alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred)[start_t:end_t, i], alpha=.5, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MSE = np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2\n",
    "print(f\"MSE = {MSE}\")\n",
    "print(f\"Normalized MSE = {MSE/(scaler_y.inverse_transform(orig).max()-scaler_y.inverse_transform(orig).min())}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction of time series sequences\n",
    "new_inputs = np.concatenate((X.T[:number_of_lines].T, scaler_x.transform(X.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = scaler_y.transform(y)\n",
    "plt.plot(scaler_y.inverse_transform(orig), alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred), alpha=.2, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"MSE = {np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2}\")\n",
    "print(f\"MSEP = {(np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2)/np.mean(scaler_y.inverse_transform(orig))}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ENSp = np.full((int(len(y)/50), 1), np.nan)\n",
    "ENSo = np.full((int(len(y)/50), 1), np.nan)\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    ENSp[i] = scaler_y.inverse_transform(pred[k:(k+50)]).sum()\n",
    "    ENSo[i] = scaler_y.inverse_transform(orig[k:(k+50)]).sum()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 10))\n",
    "ax1.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "ax1.set_ylim(0, 100)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "#ax1.set_ylim([ymin, ymax])\n",
    "ax1.hist(ENSo, alpha = 0.7)\n",
    "ax1.set_xlabel(\"ENS\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"OPF\")\n",
    "ax2.hist(ENSp, alpha = 0.7)\n",
    "ax2.set_xlabel(\"ENS\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"ANN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "print(f\"mean ENS predicted = {ENSp.mean()}\")\n",
    "print(f\"mean ENS original = {ENSo.mean()}\")\n",
    "print(f\"median ENS predicted = {np.median(ENSp)}\")\n",
    "print(f\"median ENS original = {np.median(ENSo)}\")\n",
    "print(f\"standard deviation ENS predicted = {ENSp.std()}\")\n",
    "print(f\"standard deviation ENS original = {ENSo.std()}\")\n",
    "print(f\"kurtosis ENS predicted: {kurtosis(ENSp)}\")\n",
    "print(f\"kurtosis ENS original: {kurtosis(ENSo)}\")\n",
    "print(f\"skeweness ENS predicted: {skew(ENSp)}\")\n",
    "print(f\"skeweness ENS original: {skew(ENSo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survivability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaxLoad = df_I.iloc[:,42:].sum(axis=1).max()\n",
    "#MaxLoad = 173.54 #MW (from analysed time series)\n",
    "crit_load_pu = np.linspace(0,0.45,100)\n",
    "crit_load = crit_load_pu*MaxLoad\n",
    "Sp = []\n",
    "So = []\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    Sp.append(scaler_y.inverse_transform(pred[k:(k+50)]).T[0])\n",
    "    So.append(scaler_y.inverse_transform(orig[k:(k+50)]).T[0])\n",
    "Sp = np.array(Sp)\n",
    "So = np.array(So)\n",
    "\n",
    "q = 1 # quantile deciding the maximum loss of load\n",
    "#df.iloc[:,5:][(df.type == \"load\") & (df.field == \"max_p_mw\")].sum(axis=1).max() # max load\n",
    "surv_p = np.full(len(crit_load), np.nan)\n",
    "surv_o = np.full(len(crit_load), np.nan)\n",
    "for i, c in enumerate(crit_load):\n",
    "    #surv_p[i] = (Sp.max(axis=1) < c).sum()/240\n",
    "    surv_p[i] = (np.quantile(Sp, q, axis=1) < c).sum()/240\n",
    "    #surv_o[i] = (So.max(axis=1) < c).sum()/240\n",
    "    surv_o[i] = (np.quantile(So, q, axis=1) < c).sum()/240\n",
    "#plt.plot((1-crit_load_pu), surv)\n",
    "f, (ax1) = plt.subplots(1,1, figsize=(10, 5))\n",
    "ax1.plot((1-crit_load_pu), surv_o*100)\n",
    "ax1.set_xlabel(\"Supplied Load [pu]\")\n",
    "ax1.set_ylabel(\"Survivability [%]\")\n",
    "ax1.set_title(\"Survivability\")\n",
    "ax1.plot((1-crit_load_pu), surv_p*100)\n",
    "ax1.legend([\"OPF\", \"ANN\"], loc =\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp.max(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet.save_checkpoint('model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v0.py\n",
    "#%run -i ./v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet = NeuNet(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet.load_checkpoint(\"model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "new_neunet.train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses of resumed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = new_neunet.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
