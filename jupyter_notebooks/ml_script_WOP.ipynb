{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "index1 = pd.MultiIndex.from_tuples([\n",
    "    ('California', 2000), ('California', 2010), ('California', 2020),\n",
    "    ('New York', 2000), ('New York', 2010), ('New York', 2020),\n",
    "    ('Texas', 2000), ('Texas', 2010) , ('Texas', 2020)])\n",
    "\n",
    "index2 = pd.MultiIndex.from_tuples([\n",
    "    ('California', 2000), ('California', 2010),\n",
    "    ('New York', 2000), ('New York', 2010),\n",
    "    ('Texas', 2000), ('Texas', 2010)])\n",
    "\n",
    "# Columns as requested\n",
    "columns_df1 = ['Downtown', 'Belt', 'Outskirts']\n",
    "columns_df2 = ['Belt', 'Outskirts', 'Land']\n",
    "\n",
    "# Data for df1\n",
    "data_df1 = [[1000000, 500000, 200000], \n",
    "            [1000010, 500010, 200010], \n",
    "            [1000020, 500020, 200020],\n",
    "            [5000000, 2000000, 1000000], \n",
    "            [5000010, 2000010, 1000010], \n",
    "            [5000020, 2000020, 1000020],\n",
    "            [3000000, 900000, 700000], \n",
    "            [3000010, 900010, 700010], \n",
    "            [3000020, 900020, 700020],]\n",
    "\n",
    "# Data for df2\n",
    "data_df2 = [[500000, 200000, 100000], \n",
    "            [500010, 200010, 100010], \n",
    "            [2000000, 1000000, 500000], \n",
    "            [2000010, 1000010, 500010], \n",
    "            [900000, 700000, 300010], \n",
    "            [900010, 700010, 300010],]\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = pd.DataFrame(data_df1, index=index1, columns=columns_df1)\n",
    "df2 = pd.DataFrame(data_df2, index=index2, columns=columns_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Belt</th>\n",
       "      <th>Downtown</th>\n",
       "      <th>Land</th>\n",
       "      <th>Outskirts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">California</th>\n",
       "      <th>2000</th>\n",
       "      <td>500000</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>500010</td>\n",
       "      <td>1000010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>500020</td>\n",
       "      <td>1000020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">New York</th>\n",
       "      <th>2000</th>\n",
       "      <td>2000000</td>\n",
       "      <td>5000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2000010</td>\n",
       "      <td>5000010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2000020</td>\n",
       "      <td>5000020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Texas</th>\n",
       "      <th>2000</th>\n",
       "      <td>900000</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>900010</td>\n",
       "      <td>3000010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>900020</td>\n",
       "      <td>3000020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">California</th>\n",
       "      <th>2000</th>\n",
       "      <td>500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>500010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100010.0</td>\n",
       "      <td>200010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">New York</th>\n",
       "      <th>2000</th>\n",
       "      <td>2000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500010.0</td>\n",
       "      <td>1000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Texas</th>\n",
       "      <th>2000</th>\n",
       "      <td>900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300010.0</td>\n",
       "      <td>700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>900010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300010.0</td>\n",
       "      <td>700010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Belt   Downtown      Land  Outskirts\n",
       "California 2000   500000  1000000.0       NaN     200000\n",
       "           2010   500010  1000010.0       NaN     200010\n",
       "           2020   500020  1000020.0       NaN     200020\n",
       "New York   2000  2000000  5000000.0       NaN    1000000\n",
       "           2010  2000010  5000010.0       NaN    1000010\n",
       "           2020  2000020  5000020.0       NaN    1000020\n",
       "Texas      2000   900000  3000000.0       NaN     700000\n",
       "           2010   900010  3000010.0       NaN     700010\n",
       "           2020   900020  3000020.0       NaN     700020\n",
       "California 2000   500000        NaN  100000.0     200000\n",
       "           2010   500010        NaN  100010.0     200010\n",
       "New York   2000  2000000        NaN  500000.0    1000000\n",
       "           2010  2000010        NaN  500010.0    1000010\n",
       "Texas      2000   900000        NaN  300010.0     700000\n",
       "           2010   900010        NaN  300010.0     700010"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.concat([df1, df2], sort=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI supported Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuNet(object):                                                               # to the class we shall provide a model, a loss_fn and an optimizer.\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)                                                  # here we send the model to the device\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "    def to(self, device):                                                           # this is the function sending the model to the device\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):                           # data loaders provide the input data in a sutiable format to the model, in a minibatch size\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()                                                      # the model has a different behaviour during training and evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()                                              # avoid cumulation of gradients\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()                                                       # here we set the model to evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, \n",
    "            # since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the \n",
    "        # same mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):                                             # this function execute the training of the model\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:                                                         # this is optional, i.e. Tensorboard output\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))                                 # sending input to device\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()                                          # sending back to cpu for return\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b', lw=1)\n",
    "        plt.plot(self.val_losses, label='Test Loss', c='r', lw=1)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PATH_NETWORK_WG = r\".\\network_with_gen.csv\" \n",
    "# df_I = pd.read_csv(PATH_NETWORK_WG, sep=\";\", decimal=\",\")\n",
    "# temp_df = df_I.copy()\n",
    "# arr = temp_df.to_numpy()\n",
    "# df_I = pd.DataFrame(np.tile(arr, (240, 1)), columns = temp_df.columns)\n",
    "\n",
    "PATH_MONTECARLO = r\".\\montecarlo_database.csv\" \n",
    "df_montecarlo = pd.read_csv(PATH_MONTECARLO, sep=\",\", index_col=[0, 1, 2, 3, 4])# , decimal=\",\")\n",
    "column_mapping = {col: i+1 for i, col in enumerate(df_montecarlo.columns)}\n",
    "timestep_mapping_df = pd.DataFrame(list(column_mapping.items()), columns=['time', 'timestep'])\n",
    "# number_of_strata = df_montecarlo['strata'].max()\n",
    "# number_of_iterations = df_montecarlo['iteration'].max()+1\n",
    "\n",
    "PATH_NETWORK = r\".\\network.xlsx\" \n",
    "df_network = pd.read_excel(PATH_NETWORK, sheet_name=\"profiles\", decimal=\",\")\n",
    "df_network = df_network.drop(index=0).reset_index(drop=True)\n",
    "df_network = df_network.drop(df_network.columns[0], axis=1)\n",
    "temp_df = df_network.copy()\n",
    "arr = temp_df.to_numpy()\n",
    "arr = arr.astype(np.float64)\n",
    "df_network = pd.DataFrame(np.tile(arr, (240, 1)), columns = temp_df.columns)\n",
    "\n",
    "PATH_ENGINE = r\".\\engine_database.csv\"\n",
    "df_engine = pd.read_csv(PATH_ENGINE, sep=\",\", index_col=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURRENTLY RECODED !!!\n",
    "X_df = df_engine.loc[:,:,\"in_service\",\"line\",:,:].stack().unstack(\"id\")\n",
    "number_of_lines = len(set(X_df.columns)) # number of lines\n",
    "df_network.index= X_df.index\n",
    "X_df = pd.concat([X_df, df_network], axis=1)\n",
    "###\n",
    "# Now the engine database is needed!\n",
    "###\n",
    "y_df = df_engine.loc[:,:,\"loss_of_load_p_mw\",\"load\",:,:].stack().unstack(\"id\")\n",
    "ysum_df = pd.DataFrame(y_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_montecarlo = df_montecarlo.stack().unstack(\"id\")\n",
    "df_network.index= X_df_montecarlo.index\n",
    "number_of_lines = len(set(X_df_montecarlo.columns))\n",
    "X_df_montecarlo = pd.concat([X_df_montecarlo, df_network], axis=1)\n",
    "X_df_montecarlo.insert(0, 'idx', range(1, len(X_df_montecarlo) + 1))\n",
    "X = X_df_montecarlo.to_numpy()\n",
    "\n",
    "\n",
    "z = pd.DataFrame(X).iloc[:,1:number_of_lines+1].astype(int).astype(str).agg(''.join, axis=1)\n",
    "l = []\n",
    "from collections import Counter\n",
    "c = Counter(z)\n",
    "for k in z:\n",
    "    if c[k] == 1:\n",
    "        l.append(str('G0'))\n",
    "    else:\n",
    "        l.append(str(k))\n",
    "\n",
    "X_train, X_val = train_test_split(X, train_size = 0.1, stratify = pd.DataFrame(l)[0], shuffle = True, random_state = 42)\n",
    "        \n",
    "###\n",
    "# Now the engine database is needed!\n",
    "# Step 1: Montecarlo Sampling\n",
    "# Step 2: Run OPF for selected 10% Finished!\n",
    "# Step 3: Populate Engine Database\n",
    "###\n",
    "\n",
    "# y_df = df_engine.loc[:,:,\"loss_of_load_p_mw\",\"load\",:,:].stack().unstack(\"id\")\n",
    "# ysum_df = pd.DataFrame(y_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE, KEEP FOR SAFETY\n",
    "idx_for_opf = pd.Series(pd.DataFrame(X_train)[0])\n",
    "opfs_interval = (\n",
    "    X_df_montecarlo[X_df_montecarlo['idx'].isin(idx_for_opf)]\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_4': 'time'})\n",
    "    [['iteration', 'time']]\n",
    "    .merge(timestep_mapping_df, on='time', how='left')\n",
    ")\n",
    "\n",
    "for _,row in opfs_interval[['iteration','timestep']].iterrows():\n",
    "    iterationSet = row['iteration']\n",
    "    timestep = row['timestep']\n",
    "\n",
    "opfs_interval[['iteration','timestep']]\n",
    "grouped_list = opfs_interval.groupby('iteration')['timestep'].apply(list).reset_index()\n",
    "grouped_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>timestep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 3, 13, 17, 24, 39, 42, 43, 45, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[13, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[12, 16, 27, 36, 43, 47, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 20, 22, 33, 37, 49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[20, 23, 25, 29, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>235</td>\n",
       "      <td>[20, 36, 37, 41, 47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>236</td>\n",
       "      <td>[2, 28, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>237</td>\n",
       "      <td>[10, 22, 24, 35, 38, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>238</td>\n",
       "      <td>[9, 14, 17, 23, 26, 37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>239</td>\n",
       "      <td>[15, 18, 28, 34, 39]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iteration                                timestep\n",
       "0            0  [1, 3, 13, 17, 24, 39, 42, 43, 45, 49]\n",
       "1            1                                [13, 50]\n",
       "2            2            [12, 16, 27, 36, 43, 47, 49]\n",
       "3            3              [1, 2, 20, 22, 33, 37, 49]\n",
       "4            4                    [20, 23, 25, 29, 38]\n",
       "..         ...                                     ...\n",
       "233        235                    [20, 36, 37, 41, 47]\n",
       "234        236                             [2, 28, 39]\n",
       "235        237                [10, 22, 24, 35, 38, 48]\n",
       "236        238                 [9, 14, 17, 23, 26, 37]\n",
       "237        239                    [15, 18, 28, 34, 39]\n",
       "\n",
       "[238 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_for_opf = pd.Series(X_train[:, 0])  # Assuming X_train is a NumPy array. If not, the original code works.\n",
    "\n",
    "opfs_interval = (\n",
    "    X_df_montecarlo[X_df_montecarlo['idx'].isin(idx_for_opf)]\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_4': 'time'})\n",
    "    [['iteration', 'time']]\n",
    "    .merge(timestep_mapping_df, on='time', how='left')\n",
    ")\n",
    "\n",
    "grouped_list = (\n",
    "    opfs_interval.groupby('iteration')['timestep']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grouped_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "240",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m iteration_to_timestep \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m'\u001b[39m]: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m grouped_list\u001b[38;5;241m.\u001b[39miterrows()}\n\u001b[1;32m----> 2\u001b[0m \u001b[43miteration_to_timestep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 240"
     ]
    }
   ],
   "source": [
    "iteration_to_timestep = {row['iteration']: row['timestep'] for _, row in grouped_list.iterrows()}\n",
    "iteration_to_timestep[240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: [2, 4, 50]\n",
      "--\n",
      "Iteration 1: [13, 50]\n",
      "--\n",
      "Iteration 2: [12, 16, 27, 36, 43, 47, 49]\n",
      "--\n",
      "Iteration 3: [1, 2, 20, 22, 33, 37, 49]\n",
      "--\n",
      "Iteration 4: [20, 23, 25, 29, 38]\n",
      "--\n",
      "Iteration 5: [2, 4, 11, 27, 42, 43]\n",
      "--\n",
      "Iteration 7: [4, 16, 31, 43]\n",
      "--\n",
      "Iteration 8: [16, 20, 21, 28, 38, 47, 50]\n",
      "--\n",
      "Iteration 9: [15, 19, 20, 23, 34, 41, 43]\n",
      "--\n",
      "Iteration 10: [23, 33, 34]\n",
      "--\n",
      "Iteration 11: [1, 9, 11, 31, 39]\n",
      "--\n",
      "Iteration 12: [13, 22]\n",
      "--\n",
      "Iteration 13: [20, 21, 22, 26, 32, 39, 49]\n",
      "--\n",
      "Iteration 14: [8, 15, 22, 29, 40]\n",
      "--\n",
      "Iteration 15: [4, 13, 17, 43]\n",
      "--\n",
      "Iteration 16: [1, 9, 13, 17, 37]\n",
      "--\n",
      "Iteration 17: [17, 32, 34, 48]\n",
      "--\n",
      "Iteration 18: [14, 19, 26, 33, 48]\n",
      "--\n",
      "Iteration 19: [24]\n",
      "--\n",
      "Iteration 20: [13, 20, 35]\n",
      "--\n",
      "Iteration 21: [15, 16, 17, 21, 35]\n",
      "--\n",
      "Iteration 22: [16, 20, 30]\n",
      "--\n",
      "Iteration 23: [28, 38, 42, 48]\n",
      "--\n",
      "Iteration 24: [3, 19, 28, 32]\n",
      "--\n",
      "Iteration 25: [1, 20, 29, 41]\n",
      "--\n",
      "Iteration 26: [10, 16, 27]\n",
      "--\n",
      "Iteration 27: [11, 15, 21, 23, 26, 43]\n",
      "--\n",
      "Iteration 28: [2, 5, 11, 41, 50]\n",
      "--\n",
      "Iteration 29: [11, 17, 28]\n",
      "--\n",
      "Iteration 30: [12, 30, 38, 40, 47]\n",
      "--\n",
      "Iteration 31: [14, 26, 37, 38]\n",
      "--\n",
      "Iteration 32: [3, 46]\n",
      "--\n",
      "Iteration 33: [17, 26, 32, 44]\n",
      "--\n",
      "Iteration 35: [16, 34, 38, 46, 50]\n",
      "--\n",
      "Iteration 36: [4, 9, 17, 19, 28, 33, 42]\n",
      "--\n",
      "Iteration 37: [1, 35, 38]\n",
      "--\n",
      "Iteration 38: [10, 24, 42, 49]\n",
      "--\n",
      "Iteration 39: [13, 24, 49]\n",
      "--\n",
      "Iteration 40: [14, 16, 20, 30, 36, 44]\n",
      "--\n",
      "Iteration 41: [12, 29, 48]\n",
      "--\n",
      "Iteration 42: [29]\n",
      "--\n",
      "Iteration 43: [3, 16, 24, 26, 44]\n",
      "--\n",
      "Iteration 44: [5, 24, 25, 33, 34, 37]\n",
      "--\n",
      "Iteration 45: [12, 22, 35, 39, 48]\n",
      "--\n",
      "Iteration 46: [12, 15, 22, 31, 32, 36]\n",
      "--\n",
      "Iteration 47: [22, 27, 47]\n",
      "--\n",
      "Iteration 48: [17, 42, 50]\n",
      "--\n",
      "Iteration 49: [9, 15, 20, 24, 31]\n",
      "--\n",
      "Iteration 50: [2, 4, 14, 16, 30, 37, 43]\n",
      "--\n",
      "Iteration 51: [40, 49]\n",
      "--\n",
      "Iteration 52: [4, 7, 16, 25, 36, 38, 40]\n",
      "--\n",
      "Iteration 53: [9, 41, 42]\n",
      "--\n",
      "Iteration 54: [22, 26, 40]\n",
      "--\n",
      "Iteration 55: [1, 15, 18, 25, 35, 38, 40, 43]\n",
      "--\n",
      "Iteration 56: [9, 33, 37, 48, 50]\n",
      "--\n",
      "Iteration 57: [20, 32, 35]\n",
      "--\n",
      "Iteration 58: [4, 22, 27, 35, 38, 45]\n",
      "--\n",
      "Iteration 59: [2, 44]\n",
      "--\n",
      "Iteration 60: [22, 27, 37]\n",
      "--\n",
      "Iteration 61: [18, 21, 30, 31, 40, 43]\n",
      "--\n",
      "Iteration 62: [15, 17, 28]\n",
      "--\n",
      "Iteration 63: [1, 18, 33, 37]\n",
      "--\n",
      "Iteration 64: [12, 21, 27, 32, 33, 39, 43, 44]\n",
      "--\n",
      "Iteration 65: [1, 34, 36, 48]\n",
      "--\n",
      "Iteration 66: [15, 22, 33]\n",
      "--\n",
      "Iteration 67: [12, 19, 20]\n",
      "--\n",
      "Iteration 68: [20, 24, 32, 33, 45, 47, 49]\n",
      "--\n",
      "Iteration 69: [12, 17, 28, 31]\n",
      "--\n",
      "Iteration 70: [1, 24, 29, 39, 44, 48]\n",
      "--\n",
      "Iteration 71: [21, 24, 36, 37]\n",
      "--\n",
      "Iteration 72: [12, 21, 23, 30, 31, 35, 38, 39]\n",
      "--\n",
      "Iteration 73: [4, 18]\n",
      "--\n",
      "Iteration 74: [9, 10, 35, 41, 50]\n",
      "--\n",
      "Iteration 75: [2, 18, 32]\n",
      "--\n",
      "Iteration 76: [14, 18, 26, 48]\n",
      "--\n",
      "Iteration 77: [8, 14, 22, 26, 32, 44, 45, 46, 50]\n",
      "--\n",
      "Iteration 78: [5, 14, 16]\n",
      "--\n",
      "Iteration 79: [6, 17, 18, 24, 27]\n",
      "--\n",
      "Iteration 80: [22, 24, 31, 35]\n",
      "--\n",
      "Iteration 81: [9, 20, 25, 35]\n",
      "--\n",
      "Iteration 82: [5, 18, 41]\n",
      "--\n",
      "Iteration 83: [25]\n",
      "--\n",
      "Iteration 84: [28, 32, 41]\n",
      "--\n",
      "Iteration 85: [18, 27, 29, 39, 48]\n",
      "--\n",
      "Iteration 86: [3, 6, 24, 48]\n",
      "--\n",
      "Iteration 87: [34, 37]\n",
      "--\n",
      "Iteration 88: [2, 3, 36, 39, 41, 49, 50]\n",
      "--\n",
      "Iteration 89: [2, 10, 13, 22, 26, 45, 47, 49]\n",
      "--\n",
      "Iteration 90: [18, 21, 24, 32, 38, 44, 48]\n",
      "--\n",
      "Iteration 91: [1, 13, 18, 22, 23, 33, 48, 49]\n",
      "--\n",
      "Iteration 92: [15, 16, 26, 39, 48]\n",
      "--\n",
      "Iteration 93: [11, 36]\n",
      "--\n",
      "Iteration 94: [4, 10, 18, 36, 38, 45, 46]\n",
      "--\n",
      "Iteration 95: [7, 17, 24, 44]\n",
      "--\n",
      "Iteration 96: [9, 17, 20, 33, 48]\n",
      "--\n",
      "Iteration 97: [1, 11, 47, 49]\n",
      "--\n",
      "Iteration 98: [10, 36, 46]\n",
      "--\n",
      "Iteration 99: [1, 43, 50]\n",
      "--\n",
      "Iteration 100: [9, 20, 25, 38, 46, 48]\n",
      "--\n",
      "Iteration 101: [1, 4, 31, 33, 38, 41]\n",
      "--\n",
      "Iteration 102: [11, 21, 46, 49]\n",
      "--\n",
      "Iteration 103: [10, 13, 46]\n",
      "--\n",
      "Iteration 104: [13, 20, 21, 33]\n",
      "--\n",
      "Iteration 105: [10, 16, 34, 39, 46]\n",
      "--\n",
      "Iteration 106: [15, 47, 49]\n",
      "--\n",
      "Iteration 107: [23, 24, 28, 34, 37]\n",
      "--\n",
      "Iteration 108: [10, 31, 45]\n",
      "--\n",
      "Iteration 109: [12, 13, 16, 23, 24, 39, 44, 48]\n",
      "--\n",
      "Iteration 110: [42]\n",
      "--\n",
      "Iteration 111: [22, 35, 38, 45]\n",
      "--\n",
      "Iteration 112: [5, 19, 39, 42, 45, 47]\n",
      "--\n",
      "Iteration 113: [11, 18, 35]\n",
      "--\n",
      "Iteration 114: [2, 10, 30]\n",
      "--\n",
      "Iteration 115: [10, 35]\n",
      "--\n",
      "Iteration 116: [30, 35]\n",
      "--\n",
      "Iteration 117: [9, 12, 17, 20, 41]\n",
      "--\n",
      "Iteration 118: [18, 26, 46]\n",
      "--\n",
      "Iteration 119: [3, 5, 15, 18, 24, 45]\n",
      "--\n",
      "Iteration 120: [14, 19, 29, 30]\n",
      "--\n",
      "Iteration 121: [2, 9, 22, 29, 36, 41]\n",
      "--\n",
      "Iteration 122: [3, 13, 21, 23, 32, 40]\n",
      "--\n",
      "Iteration 123: [11, 22, 24, 36, 47, 49, 50]\n",
      "--\n",
      "Iteration 124: [2, 16, 36]\n",
      "--\n",
      "Iteration 125: [2, 33, 37]\n",
      "--\n",
      "Iteration 126: [10, 18, 24, 34, 43]\n",
      "--\n",
      "Iteration 127: [4, 22, 25, 31, 43, 48]\n",
      "--\n",
      "Iteration 128: [1, 15, 28]\n",
      "--\n",
      "Iteration 129: [12, 40]\n",
      "--\n",
      "Iteration 130: [15, 19, 27, 35, 41, 46]\n",
      "--\n",
      "Iteration 131: [5, 11, 16, 27, 31, 38]\n",
      "--\n",
      "Iteration 132: [14, 22, 24, 35, 41, 45]\n",
      "--\n",
      "Iteration 133: [12, 19, 25, 30, 40, 45, 49]\n",
      "--\n",
      "Iteration 134: [10, 17, 28, 31, 43, 44, 49, 50]\n",
      "--\n",
      "Iteration 135: [11, 20, 28, 31]\n",
      "--\n",
      "Iteration 136: [15, 34]\n",
      "--\n",
      "Iteration 137: [18, 30, 35, 41, 50]\n",
      "--\n",
      "Iteration 138: [10, 19, 27]\n",
      "--\n",
      "Iteration 139: [2, 14, 19, 24, 33, 39, 45]\n",
      "--\n",
      "Iteration 140: [4, 14, 18, 28, 36, 38, 50]\n",
      "--\n",
      "Iteration 141: [17, 24, 31, 39]\n",
      "--\n",
      "Iteration 142: [2, 13, 22, 23, 28, 38, 47, 50]\n",
      "--\n",
      "Iteration 143: [14, 20, 25, 32, 40, 49]\n",
      "--\n",
      "Iteration 144: [11, 18, 26, 37, 40, 44]\n",
      "--\n",
      "Iteration 145: [15, 41]\n",
      "--\n",
      "Iteration 146: [2, 11, 22, 26, 32, 43, 49]\n",
      "--\n",
      "Iteration 147: [14, 18, 28, 36, 40, 48]\n",
      "--\n",
      "Iteration 148: [3, 5, 14, 33, 39, 47]\n",
      "--\n",
      "Iteration 149: [27, 33, 40]\n",
      "--\n",
      "Iteration 150: [5, 11, 16, 27, 34]\n",
      "--\n",
      "Iteration 151: [3, 12, 19, 26, 38]\n",
      "--\n",
      "Iteration 152: [11, 16, 28, 36, 39, 46, 48, 50]\n",
      "--\n",
      "Iteration 153: [13, 20, 28, 31, 38, 47, 49]\n",
      "--\n",
      "Iteration 154: [9, 16, 27, 36, 38, 47]\n",
      "--\n",
      "Iteration 155: [17, 28, 33]\n",
      "--\n",
      "Iteration 156: [11, 17, 26]\n",
      "--\n",
      "Iteration 157: [9, 19, 24, 31, 43, 44]\n",
      "--\n",
      "Iteration 158: [4, 14, 18, 27, 30, 34, 41]\n",
      "--\n",
      "Iteration 159: [9, 19, 28, 36, 48]\n",
      "--\n",
      "Iteration 160: [12, 21, 29, 35, 43, 47]\n",
      "--\n",
      "Iteration 161: [4, 21, 37]\n",
      "--\n",
      "Iteration 162: [14, 22, 23, 31, 43, 49]\n",
      "--\n",
      "Iteration 163: [14, 19, 24, 35, 43, 48]\n",
      "--\n",
      "Iteration 164: [13, 22, 28, 36, 38, 49]\n",
      "--\n",
      "Iteration 165: [4, 14, 17, 27, 30, 37]\n",
      "--\n",
      "Iteration 166: [14, 16, 25, 31, 43, 49]\n",
      "--\n",
      "Iteration 167: [10, 31, 37, 38, 47]\n",
      "--\n",
      "Iteration 168: [12, 21, 27, 31, 42]\n",
      "--\n",
      "Iteration 169: [4, 12, 20, 27, 35, 49]\n",
      "--\n",
      "Iteration 170: [5, 10, 17, 29, 32, 40, 49]\n",
      "--\n",
      "Iteration 171: [3, 4, 9, 22, 23, 36, 37, 46]\n",
      "--\n",
      "Iteration 172: [12, 21, 26, 32, 43]\n",
      "--\n",
      "Iteration 173: [4, 12, 26, 35]\n",
      "--\n",
      "Iteration 174: [12, 44, 46]\n",
      "--\n",
      "Iteration 175: [9, 16, 25, 35, 41, 50]\n",
      "--\n",
      "Iteration 176: [4, 18, 34]\n",
      "--\n",
      "Iteration 177: [21, 24, 41, 46, 50]\n",
      "--\n",
      "Iteration 178: [12, 17, 26, 33, 38, 44]\n",
      "--\n",
      "Iteration 179: [14, 29, 38, 46]\n",
      "--\n",
      "Iteration 180: [1, 14, 19, 29, 34, 41, 45]\n",
      "--\n",
      "Iteration 181: [11, 16, 27, 31, 45, 48]\n",
      "--\n",
      "Iteration 182: [9, 22, 26, 33, 37]\n",
      "--\n",
      "Iteration 183: [14, 17, 29, 30, 41, 48]\n",
      "--\n",
      "Iteration 184: [11, 16, 24, 33, 38, 50]\n",
      "--\n",
      "Iteration 185: [11, 20, 27, 36, 40, 49]\n",
      "--\n",
      "Iteration 186: [5, 10, 23, 36, 45]\n",
      "--\n",
      "Iteration 187: [10, 17, 23, 34, 42, 44]\n",
      "--\n",
      "Iteration 188: [14, 18, 35, 40]\n",
      "--\n",
      "Iteration 189: [15, 20, 26, 33, 49]\n",
      "--\n",
      "Iteration 190: [2, 5, 11, 22, 24, 31, 39]\n",
      "--\n",
      "Iteration 191: [15, 19, 24, 31, 41]\n",
      "--\n",
      "Iteration 192: [14, 21, 24, 31, 42, 45]\n",
      "--\n",
      "Iteration 193: [4, 16, 17, 23, 25, 42, 47]\n",
      "--\n",
      "Iteration 194: [15, 20, 27, 34, 38, 44]\n",
      "--\n",
      "Iteration 195: [4, 14, 18, 26, 34, 41, 49]\n",
      "--\n",
      "Iteration 196: [15, 16, 25, 31, 41, 47, 49]\n",
      "--\n",
      "Iteration 197: [27, 40]\n",
      "--\n",
      "Iteration 198: [14, 21, 28, 33, 37, 50]\n",
      "--\n",
      "Iteration 199: [1, 9, 17, 32, 43, 48]\n",
      "--\n",
      "Iteration 200: [10, 16, 24, 31, 43, 49]\n",
      "--\n",
      "Iteration 201: [15, 21, 23, 31, 40, 41, 44]\n",
      "--\n",
      "Iteration 202: [3, 9, 22, 23, 33, 38, 40, 43, 44, 47]\n",
      "--\n",
      "Iteration 203: [4, 15, 18, 26, 34, 48, 50]\n",
      "--\n",
      "Iteration 204: [2, 14, 19, 28, 32, 39, 46]\n",
      "--\n",
      "Iteration 205: [3, 10, 23, 32, 39, 47, 48]\n",
      "--\n",
      "Iteration 206: [12, 16, 28, 34, 43]\n",
      "--\n",
      "Iteration 207: [17, 40]\n",
      "--\n",
      "Iteration 208: [15, 22, 29, 30, 37, 44, 49, 50]\n",
      "--\n",
      "Iteration 209: [1, 11, 26, 31, 45, 47]\n",
      "--\n",
      "Iteration 210: [9, 21, 27, 33]\n",
      "--\n",
      "Iteration 211: [10, 19, 28, 34, 47, 49]\n",
      "--\n",
      "Iteration 212: [9, 12, 26]\n",
      "--\n",
      "Iteration 213: [12, 20, 27, 36]\n",
      "--\n",
      "Iteration 214: [2, 14, 22, 23, 32, 39, 50]\n",
      "--\n",
      "Iteration 215: [5, 13, 25, 31, 50]\n",
      "--\n",
      "Iteration 216: [25, 27, 36, 37, 44, 45, 46]\n",
      "--\n",
      "Iteration 217: [4, 5, 12, 17, 25, 31, 41, 47]\n",
      "--\n",
      "Iteration 218: [15, 17, 24, 33, 35, 38, 47]\n",
      "--\n",
      "Iteration 219: [10, 19, 25, 32, 40, 46]\n",
      "--\n",
      "Iteration 220: [2, 3, 4, 13, 18, 29, 36, 39]\n",
      "--\n",
      "Iteration 221: [14, 20, 26, 30, 40]\n",
      "--\n",
      "Iteration 222: [13, 18, 23, 32, 49]\n",
      "--\n",
      "Iteration 223: [13, 21, 25, 36, 40, 50]\n",
      "--\n",
      "Iteration 224: [9, 21, 28, 30, 44, 45, 46]\n",
      "--\n",
      "Iteration 225: [4, 11, 19, 25, 36, 37, 38, 39, 44, 50]\n",
      "--\n",
      "Iteration 226: [1, 2, 9, 18, 26, 33, 42]\n",
      "--\n",
      "Iteration 227: [23, 37, 40, 44, 45]\n",
      "--\n",
      "Iteration 228: [12, 38]\n",
      "--\n",
      "Iteration 229: [3, 4, 15, 16, 23, 31, 39, 46]\n",
      "--\n",
      "Iteration 230: [1, 10, 13, 28, 45, 48]\n",
      "--\n",
      "Iteration 231: [4, 5, 10, 16, 25, 33, 37, 46]\n",
      "--\n",
      "Iteration 232: [4, 5, 14, 20, 28, 33, 40, 49]\n",
      "--\n",
      "Iteration 233: [14, 22, 28, 30, 41]\n",
      "--\n",
      "Iteration 234: [14, 22, 25, 34, 41, 48]\n",
      "--\n",
      "Iteration 235: [20, 36, 37, 41, 47]\n",
      "--\n",
      "Iteration 236: [2, 28, 39]\n",
      "--\n",
      "Iteration 237: [10, 22, 24, 35, 38, 48]\n",
      "--\n",
      "Iteration 238: [9, 14, 17, 23, 26, 37]\n",
      "--\n",
      "Iteration 239: [15, 18, 28, 34, 39]\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for  _, row in grouped_list.iterrows():\n",
    "    print(f\"Iteration {row['iteration']}: {row['timestep']}\")\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE CORRECT VARIABLES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Features and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X = X_df.to_numpy()\n",
    "y = ysum_df.to_numpy()\n",
    "# labels in str form ## sum(axis=1) gives different behaviour, dependent on python version (as it seems)\n",
    "z = pd.DataFrame(X).iloc[:,:number_of_lines].astype(int).astype(str).sum(axis=1)\n",
    "l = [] #  list\n",
    "c = Counter(z)\n",
    "for k in z:\n",
    "    #print(k)\n",
    "    if c[k] == 1:\n",
    "        l.append(str('G0'))\n",
    "    else:\n",
    "        l.append(str(k))\n",
    "\n",
    "# 1104 G0 values        \n",
    "#Split data Model ANN 1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1, stratify = pd.DataFrame(l)[0], shuffle=True, random_state=42)\n",
    "    \n",
    "##Split data Model ANN2\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X[(np.array(l) != \"G0\")], y[(np.array(l) != \"G0\")], train_size=0.1, stratify = pd.DataFrame(l)[(np.array(l) != \"G0\")][0], shuffle=True,random_state=42)\n",
    "# X_train = np.append(X_train, X[(np.array(l) == \"G0\")], axis=0)\n",
    "# y_train = np.append(y_train, y[(np.array(l) == \"G0\")], axis=0)\n",
    "\n",
    "#Split data Model ANN 3\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1,  \n",
    "#                                                 shuffle=True, random_state=42)\n",
    "\n",
    "# standardize data\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train = np.concatenate((X_train.T[:number_of_lines].T, scaler_x.fit_transform(X_train.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "X_val = np.concatenate((X_val.T[:number_of_lines].T, scaler_x.transform(X_val.T[number_of_lines:].T)), axis=1).astype(float)# without fit!\n",
    "y_train = scaler_y.fit_transform(y_train).astype(float)\n",
    "y_val = scaler_y.transform(y_val).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = pd.DataFrame(X_train).iloc[:,:number_of_lines].astype(int).astype(str).sum(axis=1) # labels in str form\n",
    "l_train = [] #  list\n",
    "c_train = Counter(z_train)\n",
    "for k in z_train:\n",
    "    #print(k)\n",
    "    l_train.append(k)\n",
    "len(set(l_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v2.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "torch.manual_seed(555)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).float()\n",
    "\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "\n",
    "dataset_train = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataset_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = []\n",
    "val_loader = []\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# # K-fold Cross Validation model evaluation\n",
    "# kfold = KFold(n_splits=5, shuffle=True)\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset_train)):\n",
    "\n",
    "#     # Print\n",
    "#     print(f'FOLD {fold}')\n",
    "#     print('--------------------------------')\n",
    "\n",
    "#     # Sample elements randomly from a given list of ids, no replacement.\n",
    "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "#     # Define data loaders for training and testing data in this fold\n",
    "#     train_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train, \n",
    "#                       batch_size=256, sampler=train_subsampler))\n",
    "#     val_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train,\n",
    "#                       batch_size=256, sampler=test_subsampler))\n",
    "\n",
    "   \n",
    "train_loader.append(DataLoader(dataset=dataset_train, batch_size=256, shuffle=True)) #batch_size=256\n",
    "val_loader.append(DataLoader(dataset=dataset_val, batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(X_train.shape[1], 512), nn.ReLU(),\n",
    "                      #nn.Dropout(p=0.1),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512,512), nn.ReLU(),\n",
    "                      nn.Linear(512, 256), nn.ReLU(),\n",
    "                      nn.Linear(256, y_train.shape[1]) ).to(device)\n",
    "\n",
    "# Defines an  optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet = NeuNet(model, loss_fn, optimizer)\n",
    "neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "neunet.set_tensorboard(name=\"runs\", folder = 'machine_learning_tutorial/Test_Grid10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(neunet.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet.train(n_epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig = neunet.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neunet.val_losses[-1])\n",
    "print(neunet.losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir machine_learning_tutorial/Test_Grid10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction of validation points\n",
    "i = 0\n",
    "start_t = 50\n",
    "end_t = start_t + 24*4*5\n",
    "#new_inputs = torch.tensor(X_val).float()\n",
    "new_inputs = np.array(X_val)\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = y_val\n",
    "f, (ax) = plt.subplots(1,1, figsize=(15, 10)) \n",
    "plt.plot(scaler_y.inverse_transform(orig)[start_t:end_t, i], alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred)[start_t:end_t, i], alpha=.5, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MSE = np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2\n",
    "print(f\"MSE = {MSE}\")\n",
    "print(f\"Normalized MSE = {MSE/(scaler_y.inverse_transform(orig).max()-scaler_y.inverse_transform(orig).min())}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction of time series sequences\n",
    "new_inputs = np.concatenate((X.T[:number_of_lines].T, scaler_x.transform(X.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = scaler_y.transform(y)\n",
    "plt.plot(scaler_y.inverse_transform(orig), alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred), alpha=.2, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"MSE = {np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2}\")\n",
    "print(f\"MSEP = {(np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2)/np.mean(scaler_y.inverse_transform(orig))}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ENSp = np.full((int(len(y)/50), 1), np.nan)\n",
    "ENSo = np.full((int(len(y)/50), 1), np.nan)\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    ENSp[i] = scaler_y.inverse_transform(pred[k:(k+50)]).sum()\n",
    "    ENSo[i] = scaler_y.inverse_transform(orig[k:(k+50)]).sum()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 10))\n",
    "ax1.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "ax1.set_ylim(0, 100)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "#ax1.set_ylim([ymin, ymax])\n",
    "ax1.hist(ENSo, alpha = 0.7)\n",
    "ax1.set_xlabel(\"ENS\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"OPF\")\n",
    "ax2.hist(ENSp, alpha = 0.7)\n",
    "ax2.set_xlabel(\"ENS\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"ANN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "print(f\"mean ENS predicted = {ENSp.mean()}\")\n",
    "print(f\"mean ENS original = {ENSo.mean()}\")\n",
    "print(f\"median ENS predicted = {np.median(ENSp)}\")\n",
    "print(f\"median ENS original = {np.median(ENSo)}\")\n",
    "print(f\"standard deviation ENS predicted = {ENSp.std()}\")\n",
    "print(f\"standard deviation ENS original = {ENSo.std()}\")\n",
    "print(f\"kurtosis ENS predicted: {kurtosis(ENSp)}\")\n",
    "print(f\"kurtosis ENS original: {kurtosis(ENSo)}\")\n",
    "print(f\"skeweness ENS predicted: {skew(ENSp)}\")\n",
    "print(f\"skeweness ENS original: {skew(ENSo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survivability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaxLoad = df_I.iloc[:,42:].sum(axis=1).max()\n",
    "#MaxLoad = 173.54 #MW (from analysed time series)\n",
    "crit_load_pu = np.linspace(0,0.45,100)\n",
    "crit_load = crit_load_pu*MaxLoad\n",
    "Sp = []\n",
    "So = []\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    Sp.append(scaler_y.inverse_transform(pred[k:(k+50)]).T[0])\n",
    "    So.append(scaler_y.inverse_transform(orig[k:(k+50)]).T[0])\n",
    "Sp = np.array(Sp)\n",
    "So = np.array(So)\n",
    "\n",
    "q = 1 # quantile deciding the maximum loss of load\n",
    "#df.iloc[:,5:][(df.type == \"load\") & (df.field == \"max_p_mw\")].sum(axis=1).max() # max load\n",
    "surv_p = np.full(len(crit_load), np.nan)\n",
    "surv_o = np.full(len(crit_load), np.nan)\n",
    "for i, c in enumerate(crit_load):\n",
    "    #surv_p[i] = (Sp.max(axis=1) < c).sum()/240\n",
    "    surv_p[i] = (np.quantile(Sp, q, axis=1) < c).sum()/240\n",
    "    #surv_o[i] = (So.max(axis=1) < c).sum()/240\n",
    "    surv_o[i] = (np.quantile(So, q, axis=1) < c).sum()/240\n",
    "#plt.plot((1-crit_load_pu), surv)\n",
    "f, (ax1) = plt.subplots(1,1, figsize=(10, 5))\n",
    "ax1.plot((1-crit_load_pu), surv_o*100)\n",
    "ax1.set_xlabel(\"Supplied Load [pu]\")\n",
    "ax1.set_ylabel(\"Survivability [%]\")\n",
    "ax1.set_title(\"Survivability\")\n",
    "ax1.plot((1-crit_load_pu), surv_p*100)\n",
    "ax1.legend([\"OPF\", \"ANN\"], loc =\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp.max(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet.save_checkpoint('model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v0.py\n",
    "#%run -i ./v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet = NeuNet(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet.load_checkpoint(\"model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "new_neunet.train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses of resumed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = new_neunet.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
